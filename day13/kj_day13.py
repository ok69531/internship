### Transformer ###

# module
import torch
import torch.nn as nn
import math
from torch.autograd import Variable
from copy import deepcopy

# Tokenization
# ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë‚˜íƒ€ë‚¼ ë°©ë²•ì´ í•„ìš”í•¨
# í† í°í™” : í…ìŠ¤íŠ¸ ë¬¸ìì—´ì„ ì••ì¶•ëœ ê¸°í˜¸ì˜ ì‹œí€€ìŠ¤ë¡œ êµ¬ë¬¸ ë¶„ì„í•˜ëŠ” í”„ë¡œì„¸ìŠ¤
# ì´ í”„ë¡œì„¸ìŠ¤ì—ì„œ ê° ì •ìˆ˜ê°€ í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ì˜ ë²¡í„°ê°€ ìƒì„±ë¨
# ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ëŠ” í† í¬ë‚˜ì´ì €ëŠ” WordPieceì„ (BERTì™€ ê°™ì€ ìµœê·¼ì˜ ì–¸ì–´ ëª¨ë¸ì´ WordPiece í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•¨)
# WordPieceëŠ” ì „ì²´ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ í† í°í™”í•˜ëŠ” ê²ƒìœ¼ë¡œ ë””ì½”ë”©ì´ ë” ì‰½ê³  ì§ê´€ì ìœ¼ë¡œ ë³´ì„
from transformers import BertTokenizer
# BERTëŠ” 2018ë…„ êµ¬ê¸€ì´ ê³µê°œí•œ ì‚¬ì „ í›ˆë ¨ëœ(pretrained) ëª¨ë¸ì„
# íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì´ìš©í•´ êµ¬í˜„ë˜ì—ˆìœ¼ë©° ìœ„í‚¤í”¼ë””ì•„ì™€ BooksCorpusì™€ ê°™ì€ ë ˆì´ë¸”ì´ ì—†ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ í›ˆë ¨ë˜ì—ˆìŒ
# ê¸°ë³¸ êµ¬ì¡°ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ë¥¼ ìŒ“ì•„ì˜¬ë¦° êµ¬ì¡°ì„
tok = BertTokenizer.from_pretrained("bert-base-uncased")

#tok("Hello, how are you doing?")['inputs_ids']
tok("The Frenchman spoke in the [MASK] language and ate ğŸ¥–")['input_ids']
tok("[CLS] [SEP] [MASK] [UNK]")['input_ids']


# Embeddings
# í…ìŠ¤íŠ¸ì˜ ì ì ˆí•œ í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´, ì‹œí€€ìŠ¤ì˜ ê° ê°œë³„ í† í°ì€ ì„ë² ë”©ì„ í†µí•´ ë²¡í„°ë¡œ ë³€í™˜ë¨
# ì‹œí€€ìŠ¤ì˜ ê° ê°œë³„ í† í°ì€ ì‹ ê²½ë§ ê³„ì¸µì˜ í•œ ì¢…ë¥˜ë¡œ ë³¼ ìˆ˜ ìˆìŒ
# ì„ë² ë”©ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ê³¼ í•¨ê»˜ í•™ìŠµë˜ê¸° ë•Œë¬¸
# ì´ëŠ” ì–´íœ˜(vocabulary)ì˜ ê° ë‹¨ì–´ì— ëŒ€í•œ ë²¡í„°ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°
# ì´ëŸ¬í•œ ê°€ì¤‘ì¹˜ëŠ” í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ ì´ˆê¸°í™”ë¨
# ëª¨ë¸ E âˆˆ RvocabÃ—dmodel ì„ ì´ˆê¸°í™” í•  ë•Œ vocabì˜ í¬ê¸° ë° ëª¨ë¸(dmodel=512)ì˜ ì°¨ì›ì„ ì§€ì •í•´ì•¼ í•¨
# ë§ˆì§€ë§‰ìœ¼ë¡œ ì •ê·œí™” ë‹¨ê³„ë¡œ ê°€ì¤‘ì¹˜ì— sqrt(dmodel)ì„ ê³±í•¨
import torch
import torch.nn as nn
import math

class Embed(nn.Module):
    def __init__(self, vocab: int, d_model: int = 512):
        super(Embed, self).__init__()
        self.d_model = d_model
        self.vocab = vocab
        self.emb = nn.Embedding(self.vocab, self.d_model)
        # nn.Embedding()ì€ num_embeddings, embedding_dim ë‘ ê°€ì§€ ì¸ìë¥¼ ë°›ìŒ
        # num_embeddings : ì„ë² ë”©ì„ í•  ë‹¨ì–´ë“¤ì˜ ê°œìˆ˜, ì¦‰ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°
        # embedding_dim : ì„ë² ë”© í•  ë²¡í„°ì˜ ì°¨ì›, ì‚¬ìš©ìê°€ ì •í•´ì£¼ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì„
        
        # d_model : Transformerì˜ Encoderì™€ Decoderì—ì„œì˜ ì •í•´ì§„ ì…ë ¥ê³¼ ì¶œë ¥ì˜ í¬ê¸°ë¥¼ ì˜ë¯¸, default=512
        
        self.scaling = math.sqrt(self.d_model)

    def forward(self, x):
        return self.emb(x) * self.scaling


# Positional Encoding
# CNNì´ë‚˜ RNNê³¼ëŠ” ë‹¤ë¥´ê²Œ, íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì‹œí€€ìŠ¤ì— ì„ë² ë“œëœ í† í°ì˜ ìƒëŒ€ ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŒ
# ë”°ë¼ì„œ ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ëŒ€í•œ ì…ë ¥ ì„ë² ë”©ì— ì¸ì½”ë”©ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì´ ì •ë³´ë¥¼ ì…ë ¥í•´ì•¼í•¨
# ìƒëŒ€ ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìœ¼ë©° ì •ì ì´ê±°ë‚˜ í•™ìŠµë  ìˆ˜ ìˆìŒ
# íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ê° ìœ„ì¹˜(pos)ì— ëŒ€í•œ sin, cos ë³€í™˜ì„ ì‚¬ìš©í•¨
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int = 512, dropout: float = .1, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout) # ë ˆì´ì–´ê°„ ì—°ê²° ì¤‘ ì¼ë¶€ë¥¼ ëœë¤í•˜ê²Œ ì‚­ì œí•¨, 0.1ì´ë¯€ë¡œ 10%ë¥¼ ëœë¤í•˜ê²Œ ì‚­ì œ

        # Compute the positional encodings in log space
        pe = torch.zeros(max_len, d_model) # shape : (5000,512)
        position = torch.arange(0, max_len).unsqueeze(1) # shape : (5000, 1)
        # ìˆ˜ì‹ì—ì„œ posì— í•´ë‹¹, ë‹¨ì–´ì˜ ì‹œì 

        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.Tensor([10000.0])) / d_model))
        # ìˆ˜ì‹ì—ì„œ 10000^(2i/d_model)ì— í•´ë‹¹
        # logë¥¼ ì¼ë‹¤ê°€ expë¥¼ ë‹¤ì‹œ ì¨ì¤€ ì´ìœ ëŠ” ìˆ«ì ì˜¤ë²„ í”Œë¡œìš° ë°©ì§€ë¥¼ ìœ„í•´ log spaceì—ì„œ ì—°ì‚°í•˜ê¸° ìœ„í•¨

        pe[:, 0::2] = torch.sin(position * div_term) # sinì€ ì§ìˆ˜ ì°¨ì›(2i)ì—ì„œ ì‚¬ìš©
        pe[:, 1::2] = torch.cos(position * div_term) # così€ í™€ìˆ˜ ì°¨ì›(2i+1)ì—ì„œ ì‚¬ìš©
        # 0::2ëŠ” step=2ë¡œ 0ë¶€í„° ëê¹Œì§€

        pe = pe.unsqueeze(0) # shape : (1,5000,512)
        self.register_buffer('pe', pe) # parameterê°€ ì•„ë‹ˆë¼ bufferë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ëª©ì ìœ¼ë¡œ í™œìš©
        # backpropagationì„ ì§„í–‰í•˜ì§€ ì•Šê³ , optimizationì— ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
        # optimizer.step ì ìš© X, ì¦‰ update í•˜ì§€ ì•ŠìŒ
        # state_dictì—ëŠ” ì €ì¥ë˜ë¯€ë¡œ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‚¬ìš©í•˜ê¸°ì— ìš©ì´í•¨
        # GPUì—ì„œ ì‘ë™í•¨
        # ì‰½ê²Œ ìƒê°í•´ì„œ ëª¨ë¸ì˜ parameterë¡œ ë“±ë¡í•˜ì§€ ì•Šê¸° ìœ„í•œ í•¨ìˆ˜ë¼ê³  ë³´ë©´ ë¨
        
    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
        # Variableì€ Tensorì˜ Wrapperë¡œ ì—°ì‚° ê·¸ë˜í”„ì—ì„œ ë…¸ë“œë¡œ í‘œí˜„ë¨
        # Tensorë¥¼ ê°ì‹¸ë©°, requires_grad=Falseì´ë¯€ë¡œ ì—­ì „íŒŒ ì¤‘ì— ì´ Variableë“¤ì— ëŒ€í•œ gradientë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ìŒì„ ë‚˜íƒ€ëƒ„
        # í˜„ì¬ëŠ” ëª¨ë“  Tensorê°€ ìë™ì ìœ¼ë¡œ Variableì˜ ì„±ì§ˆì„ ê°€ì§€ê¸° ë•Œë¬¸ì— êµ³ì´ ì‚¬ìš©í•  í•„ìš” X
        return self.dropout(x)


# Multi-Head Attention

# ì°¸ê³ 
# Transformer ì´ì „ì—, Sequenceì—ì„œ í•™ìŠµí•˜ê¸° ìœ„í•œ AIì—°êµ¬ì˜ íŒ¨ëŸ¬ë‹¤ì„ì€ CNN, RNN, LSTM ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ì—ˆìŒ
# Attentionì€ Transformer ì´ì „ì— ì´ë¯¸ ëª‡ëª‡ NLP ì„±ê³¼ë¥¼ ì´ë¤˜ìœ¼ë‚˜, ê·¸ ë‹¹ì‹œì—ëŠ” Convolutionì´ë‚˜ Recurrent ì—†ì´ íš¨ìœ¨ì ì¸ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ”ì§€ê°€ ë¶ˆë¶„ëª…í–ˆìŒ

# ìš°ì„  Attentionë¶€í„° í™•ì¸
# Attention LayerëŠ” Queryì™€ Key, Valueê°„ì˜ ë§µí•‘ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ
# í…ìŠ¤íŠ¸ ìƒì„±ì˜ ë§¥ë½ ê¸°ì¤€ (ì˜ë¯¸ê°€ íŠ¹ì • NLP ì‘ìš©í”„ë¡œê·¸ë¨ì— ë”°ë¼ ë‹¬ë¼ì§€ë¯€ë¡œ ê¸°ì¤€ ì„¤ì •)
# QëŠ” ì…ë ¥ì˜ ì„ë² ë”©, Vì™€ KëŠ” target (ì¼ë°˜ì ìœ¼ë¡œ V=K)
class Attention:
    def __init__(self, dropout: float = 0.):
        super(Attention, self).__init__()
        self.dropout = nn.Dropout(dropout)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, query, key, value, mask=None):
        d_k = query.size(-1) # ì—°ì‚°ê°’ì´ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì¡°ì • ìƒìˆ˜
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
        # attention scoreê³„ì‚° (QxK.transpose)/sqrt(d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        # ë§Œì•½ maskê°€ Noneì´ ì•„ë‹ˆë¼ë©´, maskê°€ 0ì¸ ë¶€ë¶„ì€ -1e9(ë§¤ìš° í° ìŒìˆ˜, ë•Œë¡œëŠ” -inf)ë¡œ ì±„ì›€
        # ì´ë ‡ê²Œ í•˜ë©´ softmaxë¥¼ í†µê³¼í–ˆì„ ë•Œì˜ ê°’ì´ 0ì´ ë˜ë¯€ë¡œ, Attention ë§¤ì»¤ë‹ˆì¦˜ì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ
        # ì´ëŠ” í›„ì† ìœ„ì¹˜ë¥¼ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì´ cheating(ë¶€ì •í–‰ìœ„)ì„ ì €ì§€ë¥´ëŠ” ê²ƒì„ ì˜ˆë°©í•˜ê¸° ìœ„í•œ ê²ƒì„
        # ì´ë¥¼ í†µí•´ ëª¨ë¸ì€ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë ¤ í•  ë•Œ ì´ì „ ìœ„ì¹˜ì˜ ë‹¨ì–´ì—ë§Œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ ìˆ˜ ìˆìŒ

        p_attn = self.dropout(self.softmax(scores)) # attention scoreë¥¼ softmaxí•¨ìˆ˜ì— í†µê³¼ì‹œí‚´

        return torch.matmul(p_attn, value) # attention value(or context vector)ë¥¼ ì¶œë ¥
        # attention value = softmax(attention score) * V
    
    def __call__(self, query, key, value, mask=None):
        return self.forward(query, key, value, mask)
        # __call__ ì€ í´ë˜ìŠ¤ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í•¨ìˆ˜ì„
        # í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í´ë˜ìŠ¤ì˜ ê°ì²´ë¥¼ ë§Œë“¤ì–´ì•¼í•˜ëŠ”ë° callì„ ë„£ì–´ì£¼ë©´ ê·¸ë ‡ê²Œ í•  í•„ìš”ì—†ì´ ë°”ë¡œ ì„ ì–¸í•˜ëŠ” ê²ƒ ê°€ëŠ¥

# ë‹¨ì¼ Attentino LayerëŠ” í•˜ë‚˜ì˜ í‘œí˜„ë§Œì„ í—ˆìš©í•˜ë¯€ë¡œ Transformerì—ì„œëŠ” Multi-Head Attentionì´ ì‚¬ìš©ë¨
# Multi-Head Attentionì€ ì—¬ëŸ¬ë²ˆì˜ Attentionì„ ë³‘ë ¬ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•
# ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë‹¤ì¤‘ íŒ¨í„´ ë° í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ
class MultiHeadAttention(nn.Module):
    def __init__(self, h: int = 8, d_model: int = 512, dropout: float = 0.1):
        super(MultiHeadAttention, self).__init__()
        self.d_k = d_model // h # d_modelì˜ ì°¨ì›ì„ num_headsë¡œ ë‚˜ëˆ„ì–´ì„œ d_model/num_headsì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” Q,K,Vì— ëŒ€í•´ì„œ num_headsê°œì˜ ë³‘ë ¬ Attention ìˆ˜í–‰
        # ê° headì˜ ì°¨ì›ì„ hë¡œ ë‚˜ëˆ„ê¸° ë•Œë¬¸ì— ì´ ì—°ì‚°ì€ ì™„ì „í•œ ì°¨ì›ì„ ê°€ì§„ í•˜ë‚˜ì˜ Attention headë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•¨
        self.h = h # num_heads = 8, ì¦‰ 8ê°œì˜ ë³‘ë ¬ Attention
        self.attn = Attention(dropout)
        self.lindim = (d_model, d_model) # (512,512)
        self.linears = nn.ModuleList([deepcopy(nn.Linear(*self.lindim)) for _ in range(4)])
        # deepcopyëŠ” ë‚´ë¶€ ê°ì²´ë“¤ê¹Œì§€ ëª¨ë‘ copyë˜ëŠ” ê²ƒ, ì¦‰ deepcopyëŠ” ì›ë³¸ ë°°ì—´ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©
        # ModuleListì— nn.Linear(512,512) ëª¨ë“ˆì„ 4ê°œ ì €ì¥í•¨
        # ì´ë ‡ê²Œ í•´ë‘ë©´ ì´ë¥¼ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ìˆœì„œëŒ€ë¡œ iterableí•˜ê²Œ ì ‘ê·¼í•˜ì—¬ ì‚¬ìš©ì´ ê°€ëŠ¥í•¨ 

        self.final_linear = nn.Linear(*self.lindim, bias=False)
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1)
        
        query, key, value = [l(x).view(query.size(0), -1, self.h, self.d_k).transpose(1, 2) \
                             for l, x in zip(self.linears, (query, key, value))]
        # self.linearsì— 4ê°œì˜ nn.Linear(512,512)ê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ forë¬¸ì´ ì´ 4ë²ˆ ëŒì•„ê°
        # l, xë¥¼ 4ë²ˆ ë§Œë“¤ì–´ì„œ ìœ„ì™€ ê°™ì€ ì—°ì‚°ì„ í†µí•´ query, key, valueë¥¼ ê°ê° ë§Œë“¦

        nbatches = query.size(0)
        x = self.attn(query, key, value, mask=mask)
        # ê° query, key, valueë¥¼ Attention í´ë˜ìŠ¤ì— ë„£ì–´ì„œ ê³„ì‚°
        
        # Concatenate and multiply by W^O
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        # viewë‚˜ transposeì™€ ê°™ì€ í•¨ìˆ˜ëŠ” ë©”ëª¨ë¦¬ë¥¼ ë”°ë¡œ í• ë‹¹í•˜ì§€ ì•ŠëŠ” tensor ê°ì²´ ì—°ì‚°
        # ì´ëŠ” column ê¸°ì¤€ìœ¼ë¡œ ì¶•ì„ ë°”ê¿”ë²„ë ¤ì„œ ë‚˜ì¤‘ì— ë°”ë€ ì¶•ë‹¨ìœ„ í¬ì¸í„° ì—°ì‚°ì´ í•„ìš”í•  ì‹œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ
        # contiguousëŠ” ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ê³µê°„ì— ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ì—¬ ì£¼ì†Œê°’ ì—°ì†ì„±ì„ ê°€ë³€ì ì´ê²Œ ë§Œë“¤ì–´ì¤Œ
        # ë”°ë¼ì„œ ìœ„ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œ í•´ê²°
        # ì´ ê³¼ì •ë“¤ì„ í†µí•´ ë‚˜ëˆ ì¡Œë˜ headë“¤ì„ ì—°ê²°

        return self.final_linear(x) # ê°€ì¤‘ì¹˜ í–‰ë ¬ W_oì— ê³±í•´ì§€ê²Œë” ì„¤ì •
        # ìµœì¢… ê²°ê³¼ë¬¼ì€ ì¸ì½”ë”ì˜ ì…ë ¥ì´ì—ˆë˜ í–‰ë ¬ì˜ í¬ê¸°ì™€ ë™ì¼


# Residuals and Layer Normalization
# Residual connection ë° ë°°ì¹˜ ì •ê·œí™”ì™€ ê°™ì€ ê°œë…ì´ í¼í¬ë¨¼ìŠ¤ë¥¼ í–¥ìƒì‹œí‚¤ê³ , í›ˆë ¨ ì‹œê°„ì„ ë‹¨ì¶•í•˜ë©°, ë³´ë‹¤ ì‹¬ì¸µì ì¸ ë„¤íŠ¸ì›Œí¬ì˜ í›ˆë ¨ì„ ê°€ëŠ¥ì¼€ í•¨
# ë”°ë¼ì„œ ëª¨ë“  Attention Layer ë° Feed Forward Layer ë‹¤ìŒì— ì´ë¥¼ ê°–ì¶”ê³  ìˆìŒ
# ê° ë ˆì´ì–´ì— dropoutì´ ì¶”ê°€ë˜ì–´ ìˆëŠ” ì´ìœ  : ë” ë‚˜ì€ ì¼ë°˜í™”ë¥¼ ìœ„í•´ì„œ

# Normalization
# í˜„ëŒ€ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ëª¨ë¸ì€ ë³´í†µ ë°°ì¹˜ ì •ê·œí™”ë¥¼ í¬í•¨í•˜ê³  ìˆìŒ
# ê·¸ëŸ¬ë‚˜ ë°°ì¹˜ ì •ê·œí™”ëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆì— ì˜í•´ ì¢Œìš°ë˜ë¯€ë¡œ ëŒ€ì‹  ë ˆì´ì–´ ì •ê·œí™”ë¥¼ ì‚¬ìš©
# ë ˆì´ì–´ ì •ê·œí™”ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ ì‘ë”ë¼ë„ ì•ˆì •ì ì„
# ë ˆì´ì–´ ì •ê·œí™” : ê° ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ë‚˜ì˜¨ featureë¥¼ ëª¨ë“  channelì— ê±¸ì³ í•œ ë²ˆ

class LayerNorm(nn.Module):
    def __init__(self, features: int, eps: float = 1e-6):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(features))
        self.beta = nn.Parameter(torch.zeros(features))
        self.eps = eps
        # featuresë¡œ ë°›ì€ ìˆ˜ ë§Œí¼ 1ë²¡í„°ì™€ 0ë²¡í„°ë¥¼ ë§Œë“¦
        # gamma = 1ë²¡í„°ì˜ parameter
        # beta = 0ë²¡í„°ì˜ parameter
        # gammaì™€ betaëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì„ (ì—­ì „íŒŒë¥¼ í†µí•´ í•™ìŠµ)

    def forward(self, x):
        mean = x.mean(-1, keepdim=True) # ë¯¸ë‹ˆë°°ì¹˜ì˜ í‰ê· 
        std = x.std(-1, keepdim=True) # ë¯¸ë‹ˆë°°ì¹˜ì˜ ë¶„ì‚°
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
        # í‘œì¤€í¸ì°¨ê°€ 0ì¼ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ epsê°€ ì¶”ê°€ë¨
        # ìœ„ ì‹ì€ ì¼ë°˜ì ì¸ ì •ê·œí™” ì‹ì´ ì•„ë‹ˆë¼ ë ˆì´ì–´ ì •ê·œí™” ì‹ì„
        # ë¯¸ë‹ˆë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì´ìš©í•´ì„œ ì •ê·œí™” í•œ ë’¤
        # scale ë° shiftë¥¼ gamma, beta ê°’ì„ í†µí•´ ì‹¤í–‰í•¨
        
# Residual Connection
# ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ í˜„ì¬ ë ˆì´ì–´ì˜ ì¶œë ¥ì— ì¶”ê°€í•˜ëŠ” ê²ƒì„ ì˜ë¯¸
# ë„¤íŠ¸ì›Œí¬ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ íŠ¹ì • ë ˆì´ì–´ë¥¼ ê±´ë„ˆë›°ê¸° í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ëŠ” ë§¤ìš° ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í—ˆìš©í•¨
# ê·¸ë¦¼ì—ì„œ Addì— í•´ë‹¹
# ì‰½ê²Œ ìƒê°í•´ì„œ ë¸”ë¡ ê³„ì‚°(ex.Feed Forward)ì„ ê±´ë„ˆë›°ëŠ” ê²½ë¡œë¥¼ ë‘ëŠ” ê²ƒì„ ì˜ë¯¸í•¨

class ResidualConnection(nn.Module):
    def __init__(self, size: int = 512, dropout: float = .1):
        super(ResidualConnection,  self).__init__()
        self.norm = LayerNorm(size) # Layernormì˜ feature sizeê°€ sizeê°€ ë¨
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))
        # ê·¸ ë‹¤ìŒ ê° ë ˆì´ì–´ì˜ ìµœì¢… ì¶œë ¥ì´ ìœ„ returnê³¼ ê°™ìŒ


# Feed Forward
# ëª¨ë“  Attention Layerì˜ ìœ„ì— Feed Forwardê°€ ì¶”ê°€ë¨
# Feed ForwardëŠ” ReLUì™€ ë‚´ë¶€ ë ˆì´ì–´ ë“œë¡­ì•„ì›ƒì„ í†µí•´ fully-connected Layerë¡œ êµ¬ì„±ë¨
# ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ í‘œì¤€ ì°¨ì›ì€ ì…ë ¥ ë ˆì´ì–´ì˜ ê²½ìš° d_model=512, ë‚´ë¶€ ë ˆì´ì–´ì˜ ê²½ìš° d_ff=2048
# d_ff = dim_feedforward : FFN modelì˜ ì°¨ì›, FFNì˜ ì€ë‹‰ì¸µì˜ í¬ê¸° (default=2048)
# FeedForward(x) = W2max(0, xW1 + B1) + B2

class FeedForward(nn.Module):
    def __init__(self, d_model: int = 512, d_ff: int = 2048, dropout: float = .1):
        super(FeedForward, self).__init__()
        self.l1 = nn.Linear(d_model, d_ff) # nn.Linear(512,2048)
        self.l2 = nn.Linear(d_ff, d_model) # nn.Linear(2048, 512)
        self.relu = nn.ReLU() # max(0,x)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.l2(self.dropout(self.relu(self.l1(x))))


# Encoder
# ë‹¨ì¼ Encoder LayerëŠ” Feed Forwardê°€ ë’¤ë”°ë¥´ëŠ” Multi-Head Attention Layerë¡œ êµ¬ì„±ë¨
# ì—¬ê¸°ì— Residual connection ë° Layer ì •ê·œí™” ë˜í•œ í¬í•¨ë¨
# Enconding(x, mask) = FeedForward(MultiHeadAttention(x))

class EncoderLayer(nn.Module):
    def __init__(self, size: int, self_attn: MultiHeadAttention, feed_forward: FeedForward, dropout: float = .1):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn # Multi-Head Attention (Self-attention)
        self.feed_forward = feed_forward
        self.sub1 = ResidualConnection(size, dropout)
        self.sub2 = ResidualConnection(size, dropout)
        self.size = size

    def forward(self, x, mask):
        x = self.sub1(x, lambda x: self.self_attn(x, x, x, mask))
        # x, x, xëŠ” ê°ê° query, key, valueë¡œ MultiHeadAttentionì˜ ì¸ìë¡œì„œ ì‘ìš©í•¨
        # ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ì˜ xë¡œ ì¶œë ¥ë˜ê³ , ê·¸ ê°’ì— ResidualConnection ì ìš©
        # mask : Padding mask
        return self.sub2(x, self.feed_forward)
        # ìœ„ xë¥¼ Feed Forwardì™€ ê°™ì´ ResidualConnectionì„ ì ìš©ì‹œì¼œì„œ ë°˜í™˜

# ë…¼ë¬¸ì˜ ìµœì¢… Transformer EncoderëŠ” Layer ì •ê·œí™”ê°€ ë’¤ë”°ë¥´ëŠ” 6ê°œì˜ ë™ì¼í•œ Layer ì •ê·œí™”ë¡œ êµ¬ì„±ë¨
class Encoder(nn.Module):
    def __init__(self, layer, n: int = 6):
        super(Encoder, self).__init__()
        self.layers = nn.ModuleList([deepcopy(layer) for _ in range(n)])
        # layerë¡œ Encoder Layerë¥¼ ë°›ì•„ì„œ nê°œë§Œí¼ ë˜‘ê°™ì´ ìƒì„±
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
        # layersì—ì„œ layerë¥¼ í•˜ë‚˜ì”© ê°€ì ¸ì™€ì„œ layerì— xì™€ maskëŒ€ì…
        # ìµœì¢…ì ìœ¼ë¡œ ë‚˜ì˜¨ xë¥¼ Layer ì •ê·œí™”

# Decoder
# Decoding LayerëŠ” ë©”ëª¨ë¦¬ë¥¼ í¬í•¨í•œ Multi-Head Attention Layerê°€ ë’¤ë”°ë¥´ëŠ” Masked Multi-Head Attention Layerì„
# MemoryëŠ” Encoderì˜ ì¶œë ¥
# ë§ˆì§€ë§‰ìœ¼ë¡œ ì—­ì‹œ Feed Forwardë¥¼ í†µê³¼í•¨
# ë˜í•œ ëª¨ë“  êµ¬ì„±ìš”ì†ŒëŠ” Encoderì™€ ë§ˆì°¬ê°€ì§€ë¡œ Residual Connection ë° Layer ì •ê·œí™”ë¥¼ í¬í•¨í•¨
# Decoding(x, memory, mask1, mask2) = FeedForward(MultiHeadAttention(MultiHeadAttention(x, mask1), memory, mask2))

class DecoderLayer(nn.Module):
    def __init__(self, size: int, self_attn: MultiHeadAttention, src_attn: MultiHeadAttention, 
                 feed_forward: FeedForward, dropout: float = .1):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn # Masked MultiHeadAttention (Self-attention)
        self.src_attn = src_attn # MultiHeadAttention (ì¼ë°˜ attention)
        self.feed_forward = feed_forward
        self.sub1 = ResidualConnection(size, dropout) # ì²«ë²ˆì§¸ ì„œë¸Œì¸µ
        self.sub2 = ResidualConnection(size, dropout) # ë‘ë²ˆì§¸ ì„œë¸Œì¸µ
        self.sub3 = ResidualConnection(size, dropout) # ì„¸ë²ˆì§¸ ì„œë¸Œì¸µ
 
    def forward(self, x, memory, src_mask, tgt_mask):
        x = self.sub1(x, lambda x: self.self_attn(x, x, x, tgt_mask)) # ì²«ë²ˆì§¸ ì„œë¸Œì¸µ ì ìš©
        # x, x, xëŠ” ê°ê° query, key, valueë¡œ MultiHeadAttentionì˜ ì¸ìë¡œì„œ ì‘ìš©í•¨
        # tgt_mask : look-ahead mask
        # RNNì˜ ê²½ìš° ì…ë ¥ ë‹¨ì–´ë¥¼ ë§¤ ì‹œì ë§ˆë‹¤ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ìœ¼ë¯€ë¡œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì— í˜„ì¬ ì‹œì ì„ í¬í•¨í•œ ì´ì „ ì‹œì ì— ì…ë ¥ëœ ë‹¨ì–´ë“¤ë§Œ ì°¸ê³  ê°€ëŠ¥
        # but TransformerëŠ” ë¬¸ì¥ í–‰ë ¬ë¡œ ì´ë¯¸ ì…ë ¥ì„ í•œ ë²ˆì— ë°›ì•˜ìŒ
        # ë”°ë¼ì„œ í˜„ì¬ ì‹œì ì˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•  ë•Œ, ë¯¸ë˜ ì‹œì ì˜ ë‹¨ì–´ê¹Œì§€ë„ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŒ
        # ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ look-ahead maskì„
        # ë§ˆìŠ¤í‚¹ì„ í•˜ê³ ì í•˜ëŠ” ìœ„ì¹˜ì—ëŠ” 1, ë§ˆìŠ¤í‚¹ì„ í•˜ì§€ ì•ŠëŠ” ìœ„ì¹˜ì—ëŠ” 0
        x = self.sub2(x, lambda x: self.src_attn(x, memory, memory, src_mask)) # ë‘ë²ˆì§¸ ì„œë¸Œì¸µ ì ìš©
        # memoryëŠ” Encoderì˜ ì¶œë ¥, xëŠ” ì²«ë²ˆì§¸ ì„œë¸Œì¸µì˜ ì¶œë ¥
        # src_mask : Padding mask
        # Keyì— PADê°€ ìˆëŠ” ê²½ìš° í•´ë‹¹ ì—´ ì „ì²´ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê¸° ìœ„í•´ ì‚¬ìš©
        # PADì˜ ê²½ìš° softmaxë¥¼ ì§€ë‚˜ê³  ë‚˜ë©´ í•©ì´ 0ì´ ë˜ì–´ ì–´ë– í•œ ìœ ì˜ë¯¸í•œ ê°’ë„ ê°€ì§€ê³  ìˆì§€ ì•Šê²Œ ë¨
        return self.sub3(x, self.feed_forward)
        # ìœ„ xë¥¼ Feed Forwardì™€ ê°™ì´ ResidualConnectionì„ ì ìš©ì‹œì¼œì„œ ë°˜í™˜

# ìµœì¢… Encoderì—ì„œì™€ ê°™ì´, ë…¼ë¬¸ì˜ DecoderëŠ” Layer ì •ê·œí™”ê°€ ë’¤ë”°ë¥´ëŠ” 6ê°œì˜ ë™ì¼í•œ Layerë¥¼ ê°€ì§€ê³  ìˆìŒ
class Decoder(nn.Module):
    def __init__(self, layer: DecoderLayer, n: int = 6):
        super(Decoder, self).__init__()
        self.layers = nn.ModuleList([deepcopy(layer) for _ in range(n)])
        # layerë¡œ Decoder Layerë¥¼ ë°›ì•„ì„œ nê°œë§Œí¼ ë˜‘ê°™ì´ ìƒì„±
        self.norm = LayerNorm(layer.size) # Layer ì •ê·œí™”
        
    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
    # ê° Layerë§ˆë‹¤ Decoder Layer ìˆ˜í–‰ í›„ Layerì •ê·œí™”ê¹Œì§€ í•¨


# Output
# ë§ˆì§€ë§‰ìœ¼ë¡œ, Decoderì˜ ë²¡í„° ì¶œë ¥ì€ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë³€í™˜ë˜ì–´ì•¼ í•¨
# ì–¸ì–´ ë²ˆì—­ê³¼ ê°™ì€ Seq2Seq ë¬¸ì œì˜ ê²½ìš° ì¶œë ¥ì€ ê° ìœ„ì¹˜ì— ëŒ€í•œ ì´ ì–´ìœ„ì— ê´€í•œ í™•ë¥  ë¶„í¬ì„
# Fully-Connected LayerëŠ” Decoder Outputì„ logitsì˜ í–‰ë ¬ë¡œ ë³€í™˜í•˜ë©°, ì´ëŠ” targetì–´íœ˜ì˜ ì°¨ì›ì„ ê°€ì§€ê³  ìˆìŒ
# ì´ëŸ¬í•œ ìˆ«ìë¥¼ softmaxë¥¼ í†µí•´ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•¨
# ë§Œì•½ ë²ˆì—­ëœ ë¬¸ì¥ì´ 20ê°œì˜ í† í°ì„ ê°–ê³  ìˆê³ , ì´ ì–´íœ˜ëŠ” 30000ê°œì˜ í† í°ì´ë¼ê³  ê°€ì •í•˜ë©´
# ê²°ê³¼ ì¶œë ¥ì€ í–‰ë ¬ M âˆˆ R(20Ã—30000)ì´ ë¨
# ê·¸ ë‹¤ìŒ ë§ˆì§€ë§‰ ì°¨ì›ì— ëŒ€í•œ argmaxë¥¼ ì·¨í•˜ì—¬ Tokenizerë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì—´ë¡œ Decoding í•  ìˆ˜ ìˆëŠ” ì¶œë ¥ í† í° T âˆˆ N(20)ì˜ ë²¡í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
# Output(x) = LogSoftmax(max(0, xW1 + B1)) 

class Output(nn.Module):
    def __init__(self, input_dim: int, output_dim: int):
        super(Output, self).__init__()
        self.l1 = nn.Linear(input_dim, output_dim)
        self.log_softmax = nn.LogSoftmax(dim=-1) # ë” ë¹ ë¥´ê³  ìˆ˜ì¹˜ì ìœ¼ë¡œ ë” ì•ˆì •ì ì´ê¸° ë•Œë¬¸ì— LogSoftmax ì‚¬ìš©

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        logits = self.l1(x)
        return self.log_softmax(logits)


# Encoder-Decoder
# ë³´ë‹¤ ë†’ì€ ìˆ˜ì¤€ì˜ Encoder ë° Decoderì˜ í‘œí˜„ì„ í†µí•´, ìµœì¢… Encoder-Decoder ë¸”ë¡ì„ ì‰½ê²Œ ê³µì‹í™” í•  ìˆ˜ ìˆìŒ

class EncoderDecoder(nn.Module):
    def __init__(self, encoder: Encoder, decoder: Decoder, 
                 src_embed: Embed, tgt_embed: Embed, final_layer: Output):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder # Encoder
        self.decoder = decoder # Decoder
        self.src_embed = src_embed # Embed
        self.tgt_embed = tgt_embed # Embed
        self.final_layer = final_layer # Output
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        return self.final_layer(self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask))
    
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)


# Model Initialization
# ë…¼ë¬¸ì—ì„œì˜ ê²½ìš°ì™€ ê°™ì´ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ Transformerëª¨ë¸ì„ êµ¬ì¶•
# ì´ˆê¸°í™” ì „ëµì€ Xavier/Glorot ì´ˆê¸°í™” : fan inê³¼ fan outì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ í™•ë¥  ë¶„í¬ë¥¼ ì¡°ì •í•´ì¤Œ
# tanhë¥¼ activation functionìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ì—ì„œ ë§ì´ ì‚¬ìš©ë¨
# Xavier(W) ~ U[-1/n, 1/n], B=0 (ê· ì¼ ë¶„í¬, ëª¨ë“  bias 0ìœ¼ë¡œ ì´ˆê¸°í™”)

def make_model(input_vocab: int, output_vocab: int, d_model: int = 512):
    encoder = Encoder(EncoderLayer(d_model, MultiHeadAttention(), FeedForward()))
    decoder = Decoder(DecoderLayer(d_model, MultiHeadAttention(), MultiHeadAttention(), FeedForward()))
    input_embed= nn.Sequential(Embed(vocab=input_vocab), PositionalEncoding())
    output_embed = nn.Sequential(Embed(vocab=output_vocab), PositionalEncoding())
    output = Output(input_dim=d_model, output_dim=output_vocab)
    model = EncoderDecoder(encoder, decoder, input_embed, output_embed, output)
    # ì•„ë˜ ì˜ˆì‹œ ì ìš©ì‹œ input_vocab=10, output_vocab=10, d_model=512
    
    # Initialize parameters with Xavier uniform 
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model


# Tokenized symbols for source and target
src = torch.tensor([[1, 2, 3, 4, 5]]) # í† í°í™”ëœ input
src_mask = torch.tensor([[1, 1, 1, 1, 1]]) # Padding mask
tgt = torch.tensor([[6, 7, 8, 0, 0]]) # í† í°í™”ëœ output
tgt_mask = torch.tensor([[1, 1, 1, 0, 0]]) # Look-ahead mask

# Create PyTorch model
model = make_model(input_vocab=10, output_vocab=10)
# ì…ë ¥ê³¼ ì¶œë ¥ì— ëŒ€í•œ ì–´íœ˜ê°€ 10ê°œë§Œ ìˆë‹¤ê³  ê°€ì •

# Do inference and take tokens with highest probability through argmax along the vocabulary axis (-1)
result = model(src, tgt, src_mask, tgt_mask)
result
result.shape # 1,5,10
result.argmax(dim=-1)
# argmax : arguments of the maxima
# tensorì— ìˆëŠ” ëª¨ë“  elementë“¤ ì¤‘ì—ì„œ ê°€ì¥ í° ê°’ì„ ê°€ì§€ëŠ” elementì˜ indexë°˜í™˜
# dim=-1ì´ë¯€ë¡œ ë§ˆì§€ë§‰ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ maxê°’ì´ ìˆëŠ” indexë¥¼ ì¶œë ¥