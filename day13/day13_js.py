from transformers import BertTokenizer

import torch
from torch import nn
import math
from torch.autograd import Variable
from copy import deepcopy

#í† í°í™”(Tokenization)
#ë°”ì´í„° í˜ì–´ ì¸ì½”ë”©(BPE)ê°€ ì•„ë‹Œ Wordpieceë¥¼ ì‚¬ìš©
#WordPieceëŠ” ì „ì²´ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ í† í°í™”í•´ ë””ì½”ë”©ì´ ì§ê´€ì 
#ë‹¨ì–´ì˜ ì¡°ê°ìœ¼ë¡œ í† í°í™”í•˜ëŠ” BPEì²˜ëŸ¼ ì•Œ ìˆ˜ ì—†ëŠ” í† í° X
#ì‹¤ë¬´ìë“¤ì˜ ëŒ€ë¶€ë¶„ ì‚¬ì „í›ˆë ¨ëœ WordPiece í† í¬ë‚˜ì´ì € ì‚¬ìš©



tok = BertTokenizer.from_pretrained("bert-base-uncased")

#[CLS] : ì¸ì½”ë”©ì˜ ì‹œì‘ = 101
#[SEP] : ì¸ì½”ë”©ì˜ ì¢…ë£Œ ì¦‰, ë¶„ë¦¬ = 102
#[MASK] : ê¸°íƒ€ íŠ¹ìˆ˜ í† í° = 103
#[UNK] : ì•Œ ìˆ˜ ì—†ëŠ” ê¸°í˜¸(ë¹µ ì´ëª¨í‹°ì½˜ì²˜ëŸ¼) = 100

tok("Hello, how ar you doing")['input_ids']

tok("The Frenchman spoke in the [MASK] language and ate ğŸ¥–")['input_ids']

tok("[CLS] [SEP] [MASK] [UNK]")['input_ids']

# ì„ë² ë”©(Embeddings)
# í…ìŠ¤íŠ¸ì˜ í•™ìŠµì„ ìœ„í•´ ê° í† í°ë“¤ì€ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
# ì„ë² ë”©ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ê³¼ í•™ìŠµ
# ê°€ì¤‘ì¹˜ëŠ” ì •ê·œë¶„í¬ N(0,1)ì—ì„œ ì´ˆê¸°í™”
# ëª¨ë¸ì˜ ì´ˆê¸°í™” ì‹œì—ëŠ” ì–´íœ˜ì˜ í¬ê¸° ë° ëª¨ë¸ì˜ ì°¨ì›ì„ ì§€ì •
# ë§ˆì§€ë§‰ìœ¼ë¡œ ì •ê·œí™” ë‹¨ê³„ë¡œ ê°€ì¤‘ì¹˜ì— d_modelì„ ê³±í•¨

class Embed(nn.Module):
    def __init__(self, vocab: int, d_model: int = 512): #512ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì†Œê°œëœ ë…¼ë¬¸ì—ì„œ ì •í•œ ìˆ˜ì¹˜(ëª¨ë¸ ì°¨ì›)
        super(Embed, self).__init__()
        self.d_model = d_model # ëª¨ë¸ ì°¨ì› ì„¤ì •
        self.vocab = vocab # ì…ë ¥í•  ì–´íœ˜ì˜ í¬ê¸°
        self.emb = nn.Embedding(self.vocab, self.d_model) # ì–´íœ˜ë¥¼ 512ì°¨ì›ìœ¼ë¡œ ì„ë² ë”©
        self.scaling = math.sqrt(self.d_model) # ê·¼ë° ì™œ ë£¨íŠ¸ 512ë¥¼ ê³±í•´ì•¼í•¨???

    def forward(self, x):
        return self.emb(x) * self.scaling #ì„ë² ë”©ëœ ë‹¨ì–´ì˜ ì •ê·œí™”
    

#í¬ì§€ì…”ë„ ì¸ì½”ë”©(Positional Encoding)
#ìˆœí™˜ ë° í•©ì„±ê³± ì‹ ê²½ë§ê³¼ëŠ” ëŒ€ì¡°ì ìœ¼ë¡œ, ëª¨ë¸ ìì²´ëŠ” ì‹œí€€ìŠ¤ì— ì„ë² ë“œëœ í† í°ì˜ ìƒëŒ€ìœ„ì¹˜ì •ë³´ ì—†ìŒ
#ë”°ë¼ì„œ ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ëŒ€í•œ ì…ë ¥ ì„ë² ë”©ì— ì¸ì½”ë”©ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì´ ì •ë³´ë¥¼ ì…ë ¥
# ê° ìœ„ì¹˜ì— ëŒ€í•œ ì‚¼ê°í•¨ìˆ˜ ë³€í™˜ì„ ì´ìš©.
# sinì€ ì§ìˆ˜ì°¨ì›, cosëŠ” í™€ìˆ˜ ì°¨ì›ì— ì´ìš©
# ìˆ«ì ì˜¤ë²„í”Œë¡œìš° ë°©ì§€ë¥¼ ìœ„í•´ ë¡œê·¸ ê³µê°„ì—ì„œ ì—°ì‚°

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int = 512, dropout: float = .1, max_len: int = 5000): #10í¼ì„¼íŠ¸ ë…¸ë“œëŠ” ì‚¬ìš© X
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        
        # Compute the positional encodings in log space
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1) #0ë¶€í„° max_lenê¹Œì§€ ìœ„ì¹˜ë“¤ì„ í‘œí˜„í•˜ê¸° ìœ„í•˜ì—¬
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.Tensor([10000.0])) / d_model)) 
        # ì‹ì—ì„œ ë¶„ëª¨ë¶€ë¶„ ë¡œê·¸ ì”Œì› ë‹¤ê°€ ë‹¤ì‹œ exp ì¤€ê±° (ì§€ìˆ˜ì¡± ë¶„í¬ í• ë•Œì²˜ëŸ¼)
        # 2i ë¶€ë¶„ì„ arange(0, d_model, 2)ë¡œ í‘œí˜„ (0ë¶€í„° 512ê¹Œì§€ 2ì”© ë›°ë‹ˆê¹Œ)
        pe[:, 0::2] = torch.sin(position * div_term) # ì§ìˆ˜ì°¨ì›ì´ë‹ˆê¹Œ sin
        pe[:, 1::2] = torch.cos(position * div_term) # í™€ìˆ˜ì°¨ìš°ë„ˆì´ë‹ˆê¹Œ cos
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        # resgister_bufferë€?
        # Optimizerê°€ step ì¦‰, updateë¥¼ ì•ˆí•¨
        # state_dictì—ëŠ” ì €ì¥ë¨(ë‚˜ì¤‘ì— ì‚¬ìš© ê°€ëŠ¥)
        # Gpuì—ì„œ ì‘ë™

        
    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) # ê¹€ìŠ¹ê¸° í”¼ì…œ Variable ëŒ€ì‹  í† ì¹˜ ì¨ë„ ëœë‹¤
        # í™•ì¸í•´ë³¼ê²ƒ
        # Variableì€ ì˜›ë‚  í¬ì§€ì…˜ì´ì§€ë§Œ 
        #ì„ë² ë”©ëœ ê°’ì— í¬ì§€ì…”ë„ ì¸ì½”ë”© ê°’ì„ ë”í•´ ìœ„ì¹˜ ì •ë³´ í¬í•¨
        return self.dropout(x) # ë” ì¢‹ì€ ì„±ëŠ¥ì„ ìœ„í•œ dropout

# ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜(Multi-Head Attention)
# í•©ì„±ê³±ê³¼ ìˆœí™˜ ì œê±°í•˜ê³  ê³¼ê°íˆ ì–´í…ì…˜ë§Œ ì‚¬ìš©
# ì–´í…ì…˜ë ˆì´ì–´ì—ì„œ Queryì™€ Key,Value ìŒ ê°„ì˜ ë§µí•‘ì„ í•™ìŠµ
# queryëŠ” ì…ë ¥ì˜ ì„ë² ë”©, keyì™€ valueëŠ” íƒ€ê¹ƒ(ì¼ë°˜ì ìœ¼ë¡œ keyì™€ valueëŠ” ë™ì¼)
# Attention(Q,K,V) = softmax(QK'/d_k^1/2)V

class Attention:
    def __init__(self, dropout: float = 0.):
        super(Attention, self).__init__()
        self.dropout = nn.Dropout(dropout)
        self.softmax = nn.Softmax(dim=-1) #QK'/d_k^1/2ê°€ í†µê³¼í•  softmaxí•¨ìˆ˜

    def forward(self, query, key, value, mask=None):
        d_k = query.size(-1) # d_këŠ” queryì˜ ì‚¬ì´ì¦ˆë¡œ d_model / num_heads, ë…¼ë¬¸ì—ì„  ê°ê° 512ì™€ 8ì„ ì‚¬ìš©
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) #attention score ê³µì‹ì„ í‘œí˜„í•œ ê²ƒ
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9) #maskê°€ 0ì¸ ë¶€ë¶„ì„ ì•„ì£¼ ì‘ì€ ê°’ìœ¼ë¡œ, -inf ì¨ë„ ëœë‹¤~
        #ë§ˆìŠ¤í¬ëŠ” ëª¨ë¸ì˜ ë¶€ì •í–‰ìœ„ ì˜ˆë°©(ë‹¤ìŒ í† í° ì˜ˆì¸¡í•  ë•Œ ì´ì „ ìœ„ì¹˜ì˜ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ë„ë¡)
        p_attn = self.dropout(self.softmax(scores)) #softmaxí•¨ìˆ˜ì— QK'/d_k^1/2 í†µê³¼ì‹œí‚´
        return torch.matmul(p_attn, value) # ìµœì¢…ì ìœ¼ë¡œ Vì™€ ê³±í•´ì„œ attention matrix íšë“
    
    def __call__(self, query, key, value, mask=None): #callì€ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì²˜ëŸ¼ í´ë˜ìŠ¤ì˜ ê°ì²´ë„ í˜¸ì¶œí•˜ê²Œ í•´ì¤Œ!
        return self.forward(query, key, value, mask)


# multihead
# í•œ ë²ˆì˜ ì–´í…ì…˜ ë³´ë‹¤ ì—¬ëŸ¬ ë²ˆì˜ ë³‘ë ¬ì ì¸ ì–´í…ì…˜ì´ ë” íš¨ê³¼ì 
# ê° ì–´í…ì…˜ í—¤ë“œê°€ ë‹¤ë¥¸ ì‹œê°ì—ì„œ ë‹¨ì–´ ê°„ì˜ ì—°ê´€ì„±ì„ íŒŒì•…
# ì°¨ì›ì„ ì¶•ì†Œì‹œì¼œ num_heads ë§Œí¼ì˜ ë³‘ë ¬ ì—°ì‚°
class MultiHeadAttention(nn.Module):
    def __init__(self, h: int = 8, d_model: int = 512, dropout: float = 0.1):
        super(MultiHeadAttention, self).__init__()
        self.d_k = d_model // h #ì•ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ d_kë¥¼ 512/8ë¡œ ì •ì˜
        self.h = h #num_heads=8 ì„ hë¡œ ì •ì˜
        self.attn = Attention(dropout) # ì‘ì„±í–ˆë˜ attentioní´ë˜ìŠ¤ë¥¼ attnìœ¼ë¡œ ì •ì˜ (ìš°ë¦¬ê°€ trainí• ë•Œ model = class í–ˆë˜ê±°ë‘ ê°™ìŒ)
        self.lindim = (d_model, d_model) # 512*512 í–‰ë ¬ ì •ì˜ (íŠ¸ëœìŠ¤í¬ë¨¸ ì„¤ëª…ì—ì„œ ë´¤ë˜ ì •ì‚¬ê°í˜• ë„¤ëª¨ í–‰ë ¬ ìƒê°í•˜ë©´ ëœë‹¤)
        self.linears = nn.ModuleList([deepcopy(nn.Linear(*self.lindim)) for _ in range(4)])
        # nn.ModuleListëŠ” ëª¨ë“ˆì˜ ì¡´ì¬ë¥¼ pytorchì—ê²Œ ëŒë ¤ì¤Œ
        # ê·¸ëƒ¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ë²„ë¦¬ë©´ pytorchê°€ ëª°ë¼ì„œ hyperparameterê°€ ëª¨ë“ˆì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì˜¤ë¥˜ë¥¼ ë±‰ìŒ
        # ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ëª¨ë¸ì´ ë°›ëŠ” ì¸í’‹ì´ ì„œë¡œ ë‹¤ë¥´ì§€ë§Œ ë°˜ë³µí•´ì„œ ì •ì˜í•´ì•¼í•  ë•Œ ì‚¬ìš©
        # attentionì—ì„  512*512ì™€ query, key, value 4ê°œë¥¼ ì •ì˜í•´ì•¼í•˜ë‹ˆê¹Œ forë¬¸ìœ¼ë¡œ 4ë²ˆ ë°˜ë³µ
        # ì„œë¡œ ê¸¸ì´ê°€ ë‹¤ë¥´ë¯€ë¡œ *ë¥¼ ì‚¬ìš©í•´ ê°€ë³€ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¤¬ì–´ìš©
        self.final_linear = nn.Linear(*self.lindim, bias=False) # ë§ˆì°¬ê°€ì§€ë¡œ ì—¬ëŸ¬ ê°œ ë°›ìœ¼ë ¤êµ¬ *ë¥¼ ì‚¬ìš©í•´ ì •ì˜
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1) #unsqueezeë¡œ ì°¨ì›ì„ ë§ì¶°ì£¼ì
        
        # ê°€ì¤‘ì¹˜ì— ê³±í•´ì§ˆ query, key, valueë¥¼ forë¬¸ìœ¼ë¡œ ìƒì„±í•˜ê³  ê³±í•˜ê¸° ìœ„í•´ ì „ì¹˜
        query, key, value = [l(x).view(query.size(0), -1, self.h, self.d_k).transpose(1, 2) \
                             for l, x in zip(self.linears, (query, key, value))]
        nbatches = query.size(0)
        x = self.attn(query, key, value, mask=mask) #ë§Œë“¤ì–´ë‚¸ query, key, valueë¡œ ì–´í…ì…˜í•´ì„œ
        #xì— ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŒ
        
        # Concatenate and multiply by W^O
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.final_linear(x)
#transposeëŠ” ê¸°ë³¸ ë©”ëª¨ë¦¬ ì €ì¥ì†Œë¥¼ ì›ë˜ì˜ tensorì™€ ê³µìœ í•˜ê¸° ë•Œë¬¸ì— .contiguous ë°©ë²•ì€ .transpose ë‹¤ìŒì— ì¶”ê°€ë©ë‹ˆë‹¤.
#ì´ í›„ .viewë¥¼ í˜¸ì¶œí•˜ë ¤ë©´ ì¸ì ‘í•œ(contiguous) tensorê°€ í•„ìš”í•©ë‹ˆë‹¤ (ë¬¸ì„œ). .view ë°©ë²•ì€ íš¨ìœ¨ì ì¸ ë³€í™˜(reshaping), ìŠ¬ë¼ì´ì‹±(slicing) ë° ìš”ì†Œë³„ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤



# ë ˆì§€ë“€ì–¼ ë° ë ˆì´ì–´ ì •ê·œí™”
# ë ˆì§€ë“€ì–¼ ì»¤ë„¥ì…˜ ë° ì •ê·œí™”ê°€ í¼í¬ë¨¼ìŠ¤ í–¥ìƒ ë° í›ˆë ¨ ì‹œê°„ ë‹¨ì¶•ì— ì˜í–¥ì„ ì¤Œ
# ë” ë‚˜ì€ ì¼ë°˜í™”ë¥¼ ìœ„í•´ ê° ë ˆì´ì–´ì— ë“œë¡­ì•„ì›ƒ ì¶”ê°€

# ì •ê·œí™”
# í˜„ëŒ€ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì€ ë°°ì¹˜ ì •ê·œí™” í¬í•¨
# ì´ëŸ° ì •ê·œí™” ìœ í˜•ì€ ìˆœí™˜ì— ì í•©í•˜ì§€ ì•ŠìŒ
# ë ˆì´ì–´ ì •ê·œí™” ì—°ì‚°ì„ ìœ„í•´ ë¯¸ë‹ˆë°°ì¹˜ì˜ ê° ìƒ˜í”Œì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ë³„ë„ë¡œ ê³„ì‚°
class LayerNorm(nn.Module):
    def __init__(self, features: int, eps: float = 1e-6):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(features))
        self.beta = nn.Parameter(torch.zeros(features))
        self.eps = eps
# ì •ê·œí™” ê³µì‹ì— í•„ìš”í•œ ê°ë§ˆì™€ ë² íƒ€ë¥¼ ë¯¸ë¦¬ onesì™€ zerosë¡œ ìƒì„±
# betaëŠ” biasì´ë¯€ë¡œ 0ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ê²Œ ì ì ˆ
# epsëŠ” í‘œì¤€í¸ì°¨ê°€ 0ì¼ ê²½ìš° ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ 

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta #ì£¼ì„ìœ¼ë¡œ ì‹ ì ì§„ ëª»í•˜ì§€ë§Œ ì´ê±° ê·¸ëƒ¥ ê³„ì‚°ë§Œ í•œê±°!

#ë ˆì§€ë“€ì–¼
#ë ˆì§€ë“€ì–¼ ì»¤ë„¥ì…˜ì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ í˜„ì¬ ë ˆì´ì–´ì˜ ì¶œë ¥ì— ì¶”ê°€í•˜ëŠ” ê²ƒì„ ì˜ë¯¸
#íŠ¹ì • ë ˆì´ì–´ ê±´ë„ˆë›°ê¸°ê°€ ê°€ëŠ¥í•´ì ¸ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í—ˆìš©
#ê° ë ˆì´ì–´ì˜ ìµœì¢…ì¶œë ¥ = x + dropout(subLayer(LayerNorm(x)))
class ResidualConnection(nn.Module):
    def __init__(self, size: int = 512, dropout: float = .1):
        super(ResidualConnection,  self).__init__()
        self.norm = LayerNorm(size)
        # í•„ìš”í•œ ë“œë¡­ì•„ì›ƒ ì •ì˜
        self.dropout = nn.Dropout(dropout)
# ì—­ì‹œ í¬ì›Œë“œì—ì„œ ê° ë ˆì´ì–´ì˜ ìµœì¢…ì¶œë ¥ ê³µì‹ì„ ì ìš©í•´ì¤Œ
    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))

#í”¼ë“œ í¬ì›Œë“œ
# ëª¨ë“  ì–´í…ì…˜ ë ˆì´ì–´ì˜ ìœ„ì— í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì¶”ê°€
# Reluí™œì„±í™”ì™€ ë‚´ë¶€ ë ˆì´ì–´ ë“œë¡­ì•„ì›ƒì„ í†µí•´ fully-connectedë ˆì´ì–´ë¡œ êµ¬ì„±
# ì…ë ¥ ë ˆì´ì–´ ì°¨ì› : 512 vs ë‚´ë¶€ ë ˆì´ì–´ ì°¨ì› : 2048
class FeedForward(nn.Module):
    def __init__(self, d_model: int = 512, d_ff: int = 2048, dropout: float = .1):
        super(FeedForward, self).__init__()
        self.l1 = nn.Linear(d_model, d_ff)
        self.l2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU() #ë ë£¨ í™œì„±í™”ì— í•„ìš”
        self.dropout = nn.Dropout(dropout) # ë‚´ë¶€ ë ˆì´ì–´ ë“œë¡­ì•„ì›ƒ

    #feed-forward êµ¬í˜„
    def forward(self, x):
        return self.l2(self.dropout(self.relu(self.l1(x))))
    

#ì¸ì½”ë” - ë””ì½”ë”
#ì•ì—ì„œ êµ¬í˜„í–ˆë˜ ê° ì¸µì—ì„œì˜ classë¥¼ í™œìš©í•˜ì—¬
#ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì—­í• ì— ë§ê²Œ ìˆœì„œë¥¼ ì¡°ì •í•´ì¤€ë‹¤.
#ì´ ê³¼ì •ì´ í—·ê°ˆë¦´ ê²½ìš° ì „ì²´ ë„ì‹ë„ë¥¼ íŒŒì•…í•˜ì
#self-attentionê³¼ attentionì´ ì–´ëŠ ìœ„ì¹˜ì—ì„œ ë°œìƒí•˜ëŠ”ì§€ í™•ì‹¤íˆ ì¸ì§€í•˜ì!!!

#ì¸ì½”ë”©
# ì¸ì½”ë” ë ˆì´ì–´ëŠ” í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ê°€ ë’¤ë”°ë¥´ëŠ” ë‹¤ì¤‘-í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´
# ë ˆì§€ë“€ì–¼ ì»¤ë„¥ì…˜ ë° ë ˆì´ì–´ ì •ê·œí™” ë˜í•œ í¬í•¨
class EncoderLayer(nn.Module):
    def __init__(self, size: int, self_attn: MultiHeadAttention, feed_forward: FeedForward, dropout: float = .1):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sub1 = ResidualConnection(size, dropout)
        self.sub2 = ResidualConnection(size, dropout)
        self.size = size

    def forward(self, x, mask):
        x = self.sub1(x, lambda x: self.self_attn(x, x, x, mask))
        return self.sub2(x, self.feed_forward)

class Encoder(nn.Module):
    def __init__(self, layer, n: int = 6):
        super(Encoder, self).__init__()
        self.layers = nn.ModuleList([deepcopy(layer) for _ in range(n)]) #nê°œë§Œí¼, ì•ì—ì„œ ì‚¬ìš©í•œ í…Œí¬ë‹‰
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)

# ë””ì½”ë”©
# ë””ì½”ë”© ë ˆì´ì–´ëŠ” ë©”ëª¨ë¦¬ë¥¼ í¬í•¨í•œ ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´ê°€ ë’¤ë”°ë¥´ëŠ” ë§ˆìŠ¤í‚¹ëœ ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´
# ë©”ëª¨ë¦¬ëŠ” ì¸ì½”ë”ì˜ ì¶œë ¥
# ë§ˆì§€ë§‰ìœ¼ë¡œ í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¥¼ í† ì˜¤ê°€
# ëª¨ë“  êµ¬ì„±ìš”ì†ŒëŠ” ì•ì—ì„œ ë§Œë“¤ì—ˆë˜ ë ˆì§€ë“€ì–¼ ì»¤ë„¥ì…˜ ë° ë ˆì´ì–´ ì •ê·œí™”ë¥¼ í¬í•¨
from torch import nn
from copy import deepcopy
class DecoderLayer(nn.Module):
    def __init__(self, size: int, self_attn: MultiHeadAttention, src_attn: MultiHeadAttention, 
                 feed_forward: FeedForward, dropout: float = .1):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sub1 = ResidualConnection(size, dropout)
        self.sub2 = ResidualConnection(size, dropout)
        self.sub3 = ResidualConnection(size, dropout)
 
    def forward(self, x, memory, src_mask, tgt_mask):
        x = self.sub1(x, lambda x: self.self_attn(x, x, x, tgt_mask)) #ì…ë ¥ ì´í•´
        x = self.sub2(x, lambda x: self.src_attn(x, memory, memory, src_mask)) # ì…ë ¥ ì´í•´
        return self.sub3(x, self.feed_forward)

class Decoder(nn.Module):
    def __init__(self, layer: DecoderLayer, n: int = 6):
        super(Decoder, self).__init__()
        self.layers = nn.ModuleList([deepcopy(layer) for _ in range(n)])
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)

# ì¸ì½”ë”-ë””ì½”ë”
from torch import nn
class EncoderDecoder(nn.Module):
    def __init__(self, encoder: Encoder, decoder: Decoder, 
                 src_embed: Embed, tgt_embed: Embed, final_layer: Output):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.final_layer = final_layer
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        return self.final_layer(self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask))
    
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)

# ìµœì¢… ì¶œë ¥
class Output(nn.Module):
    def __init__(self, input_dim: int, output_dim: int):
        super(Output, self).__init__()
        self.l1 = nn.Linear(input_dim, output_dim)
        self.log_softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        logits = self.l1(x)
        return self.log_softmax(logits)

# ëª¨ë¸ ì´ˆê¸°í™”
# ë…¼ë¬¸ì—ì„œì™€ ê°™ì´ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬ì¶•
# ì´ˆê¸°í™” ì „ëµì€ xavier/glorot ì´ˆê¸°í™”ì´ë©° U[-1/n,-1/n] ë²”ìœ„ì˜ ê· ì¼ ë¶„í¬ì—ì„œ ì„ íƒ
# ëª¨ë“  biasëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”
def make_model(input_vocab: int, output_vocab: int, d_model: int = 512):
    encoder = Encoder(EncoderLayer(d_model, MultiHeadAttention(), FeedForward()))
    decoder = Decoder(DecoderLayer(d_model, MultiHeadAttention(), MultiHeadAttention(), FeedForward()))
    input_embed= nn.Sequential(Embed(vocab=input_vocab), PositionalEncoding())
    output_embed = nn.Sequential(Embed(vocab=output_vocab), PositionalEncoding())
    output = Output(input_dim=d_model, output_dim=output_vocab)
    model = EncoderDecoder(encoder, decoder, input_embed, output_embed, output)
    
    # Initialize parameters with Xavier uniform 
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model

# ì´í•¨ìˆ˜ëŠ” seq2seq ë¬¸ì œì— ëŒ€í•´ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” pytorch ëª¨ë¸ ë°˜í™˜
# í† í°í™”ëœ ì…ë ¥ ë° ì¶œë ¥ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì˜ˆì‹œ ì¡´ì¬

# Tokenized symbols for source and target.
src = torch.tensor([[1, 2, 3, 4, 5]])
src_mask = torch.tensor([[1, 1, 1, 1, 1]])
tgt = torch.tensor([[6, 7, 8, 0, 0]])
tgt_mask = torch.tensor([[1, 1, 1, 0, 0]])

# Create PyTorch model
model = make_model(input_vocab=10, output_vocab=10)
# Do inference and take tokens with highest probability through argmax along the vocabulary axis (-1)
result = model(src, tgt, src_mask, tgt_mask)
result.argmax(dim=-1)

#ê²°ê³¼ : tensor([[6, 6, 4, 3, 6]])

# ì´ ì‹œì ì—ì„œ ëª¨ë¸ì€ ê· ì¼í•™ ã…”ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ë¯€ë¡œ
# ì¶œë ¥ì€ íƒ€ê¹ƒê³¼ì˜ ê±°ë¦¬ëŠ” ìƒë‹¹íˆ ë©€ë‹¤.
# ì´ëŸ¬í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ê²ƒì€ ëª¹ì‹œ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ
# ë³´í†µ ì‚¬ì „ í›ˆë ¨ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì‘ìš©í•˜ë‚Ÿ.